# ƒê·ªÅ C∆∞∆°ng √în T·∫≠p Data Mining Chi Ti·∫øt

T√†i li·ªáu n√†y t·ªïng h·ª£p l√Ω thuy·∫øt c·ªët l√µi v√† h∆∞·ªõng d·∫´n gi·∫£i chi ti·∫øt c√°c d·∫°ng b√†i t·∫≠p, bao g·ªìm c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát v√† v√≠ d·ª• c·ª• th·ªÉ.

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

M·ªói ch∆∞∆°ng c√≥ 2 file ch√≠nh:
- **`huong-dan-chuong-XX.md`**: L√Ω thuy·∫øt chi ti·∫øt, v√≠ d·ª• v√† b√†i t·∫≠p c√≥ l·ªùi gi·∫£i
- **`tong-hop-cong-thuc.md`**: T·ªïng h·ª£p c√¥ng th·ª©c nhanh v·ªõi v√≠ d·ª• minh h·ªça

---

## üìò Ch∆∞∆°ng 02: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu (Data Preprocessing)

### üìñ File tham kh·∫£o:
- [H∆∞·ªõng d·∫´n chi ti·∫øt](chuong-02/huong-dan-chuong-02.md)
- [T·ªïng h·ª£p c√¥ng th·ª©c](chuong-02/tong-hop-cong-thuc.md)

### 1. Th·ªëng K√™ M√¥ T·∫£ (Descriptive Statistics)
Cho t·∫≠p d·ªØ li·ªáu $X = \{x_1, x_2, ..., x_n\}$ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp tƒÉng d·∫ßn.

*   **Mean (Trung b√¨nh):**
    $$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

*   **Median (Trung v·ªã):** Gi√° tr·ªã n·∫±m gi·ªØa t·∫≠p d·ªØ li·ªáu.
    *   **Tr∆∞·ªùng h·ª£p $n$ l·∫ª:** Median l√† ph·∫ßn t·ª≠ ·ªü v·ªã tr√≠ th·ª© $(n+1)/2$.
        $$Median = x_{(n+1)/2}$$
    *   **Tr∆∞·ªùng h·ª£p $n$ ch·∫µn:** Median l√† trung b√¨nh c·ªông c·ªßa 2 ph·∫ßn t·ª≠ ·ªü gi·ªØa (v·ªã tr√≠ $n/2$ v√† $n/2 + 1$).
        $$Median = \frac{x_{n/2} + x_{n/2 + 1}}{2}$$

*   **Mode (Y·∫øu v·ªã):** Gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu l·∫ßn nh·∫•t trong t·∫≠p d·ªØ li·ªáu.
    *   N·∫øu c√≥ nhi·ªÅu gi√° tr·ªã c√πng t·∫ßn su·∫•t cao nh·∫•t $\rightarrow$ T·∫≠p d·ªØ li·ªáu ƒëa y·∫øu v·ªã (Multimodal).

*   **Quartiles (T·ª© ph√¢n v·ªã):** Chia d·ªØ li·ªáu th√†nh 4 ph·∫ßn b·∫±ng nhau.
    *   **Q2 (Median):** T√≠nh nh∆∞ tr√™n.
    *   **Q1 (T·ª© ph√¢n v·ªã th·ª© nh·∫•t):** Median c·ªßa n·ª≠a ƒë·∫ßu d·ªØ li·ªáu (c√°c gi√° tr·ªã nh·ªè h∆°n Q2).
    *   **Q3 (T·ª© ph√¢n v·ªã th·ª© ba):** Median c·ªßa n·ª≠a sau d·ªØ li·ªáu (c√°c gi√° tr·ªã l·ªõn h∆°n Q2).
    *   *L∆∞u √Ω:* Khi t√≠nh Q1, Q3, n·∫øu $n$ l·∫ª, th∆∞·ªùng kh√¥ng bao g·ªìm Q2 v√†o n·ª≠a ƒë·∫ßu/n·ª≠a sau.

*   **Standard Deviation (ƒê·ªô l·ªách chu·∫©n):**
    $$\sigma = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n}}$$

### 2. Tr·ª±c Quan H√≥a & Outliers (Boxplot)
*   **IQR (Interquartile Range):** Kho·∫£ng tr·∫£i gi·ªØa.
    $$IQR = Q3 - Q1$$
*   **X√°c ƒë·ªãnh Outliers (Ngo·∫°i l·ªá):**
    *   **Bi√™n d∆∞·ªõi (Lower Fence):** $LF = Q1 - 1.5 \times IQR$
    *   **Bi√™n tr√™n (Upper Fence):** $UF = Q3 + 1.5 \times IQR$
    *   B·∫•t k·ª≥ gi√° tr·ªã n√†o $< LF$ ho·∫∑c $> UF$ ƒë·ªÅu l√† Outlier.

### 3. L√†m M·ªãn D·ªØ Li·ªáu (Data Smoothing) - Binning
**Ph∆∞∆°ng ph√°p Bin Means (Trung b√¨nh Bin):**
1.  **S·∫Øp x·∫øp** d·ªØ li·ªáu tƒÉng d·∫ßn.
2.  **Ph√¢n chia (Partition):** Chia d·ªØ li·ªáu v√†o c√°c bin (th√πng).
    *   *Equal-depth (frequency):* M·ªói bin c√≥ s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ b·∫±ng nhau. (Th∆∞·ªùng d√πng).
    *   *Equal-width:* Kho·∫£ng gi√° tr·ªã m·ªói bin b·∫±ng nhau.
3.  **L√†m m·ªãn:** Thay th·∫ø T·∫§T C·∫¢ gi√° tr·ªã trong m·ªói bin b·∫±ng gi√° tr·ªã TRUNG B√åNH c·ªßa bin ƒë√≥.

### 4. Chu·∫©n H√≥a D·ªØ Li·ªáu (Normalization)

*   **Min-Max Normalization:** ƒê∆∞a gi√° tr·ªã $v$ v·ªÅ kho·∫£ng $[new\_min, new\_max]$ (th∆∞·ªùng l√† $[0, 1]$).
    $$v' = \frac{v - min_{old}}{max_{old} - min_{old}} \times (new\_max - new\_min) + new\_min$$
    *   *V√≠ d·ª•:* $v=35, min=13, max=70$, ƒë∆∞a v·ªÅ $[0, 1]$.
        $$v' = \frac{35 - 13}{70 - 13} \times (1 - 0) + 0 = \frac{22}{57} \approx 0.386$$

*   **Z-Score Normalization:** D√πng khi kh√¥ng bi·∫øt r√µ min/max ho·∫∑c c√≥ outliers.
    $$v' = \frac{v - \mu}{\sigma}$$
    *   $\mu$: Trung b√¨nh.
    *   $\sigma$: ƒê·ªô l·ªách chu·∫©n.
    *   *Bi·∫øn th·ªÉ (D√πng Mean Absolute Deviation - MAD):* $v' = \frac{v - \mu}{MAD}$ v·ªõi $MAD = \frac{1}{n}\sum |x_i - \bar{x}|$.

*   **Decimal Scaling:** Di chuy·ªÉn d·∫•u ph·∫©y ƒë·ªông.
    $$v' = \frac{v}{10^j}$$
    *   $j$ l√† s·ªë nguy√™n nh·ªè nh·∫•t sao cho gi√° tr·ªã tuy·ªát ƒë·ªëi l·ªõn nh·∫•t c·ªßa t·∫≠p d·ªØ li·ªáu sau khi chia nh·ªè h∆°n 1 ($|v'| < 1$).
    *   *V√≠ d·ª•:* T·∫≠p d·ªØ li·ªáu c√≥ gi√° tr·ªã l·ªõn nh·∫•t l√† 980 $\rightarrow$ Chia cho 1000 ($j=3$) $\rightarrow 0.98$.

---

## üìô Ch∆∞∆°ng 03: Lu·∫≠t K·∫øt H·ª£p (Association Rules)

### üìñ File tham kh·∫£o:
- [H∆∞·ªõng d·∫´n chi ti·∫øt](chuong-03/huong-dan-chuong-03.md)
- [T·ªïng h·ª£p c√¥ng th·ª©c](chuong-03/tong-hop-cong-thuc.md)

### 1. C√°c ƒê·ªô ƒêo C∆° B·∫£n

*   **Support (ƒê·ªô h·ªó tr·ª£):**
    $$Support(X) = \frac{count(X)}{total\_transactions}$$
    $$Support(X \rightarrow Y) = Support(X \cup Y)$$

*   **Confidence (ƒê·ªô tin c·∫≠y):**
    $$Confidence(X \rightarrow Y) = \frac{Support(X \cup Y)}{Support(X)}$$

*   **Lift (ƒê·ªô n√¢ng):**
    $$Lift(X \rightarrow Y) = \frac{Confidence(X \rightarrow Y)}{Support(Y)}$$
    *   Lift > 1: T∆∞∆°ng quan d∆∞∆°ng
    *   Lift = 1: ƒê·ªôc l·∫≠p
    *   Lift < 1: T∆∞∆°ng quan √¢m

### 2. Thu·∫≠t To√°n Apriori (Chi ti·∫øt Join & Prune)
T√¨m t·∫≠p ph·ªï bi·∫øn v·ªõi $min\_sup$.
*   **B∆∞·ªõc Join ($L_{k-1} \bowtie L_{k-1}$):**
    *   K·∫øt h·ª£p hai t·∫≠p m·ª•c trong $L_{k-1}$ n·∫øu ch√∫ng gi·ªëng nhau ·ªü $k-2$ ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.
    *   *V√≠ d·ª•:* $L_2 = \{ \{A, B\}, \{A, C\}, \{A, E\}, \{B, C\} \}$
        *   Join $\{A, B\}$ v√† $\{A, C\} \rightarrow \{A, B, C\}$ (Candidate $C_3$).
        *   Join $\{A, B\}$ v√† $\{A, E\} \rightarrow \{A, B, E\}$.
*   **B∆∞·ªõc Prune (C·∫Øt t·ªâa):**
    *   V·ªõi m·ªói candidate $c \in C_k$, ki·ªÉm tra T·∫§T C·∫¢ t·∫≠p con k√≠ch th∆∞·ªõc $k-1$ c·ªßa n√≥.
    *   N·∫øu c√≥ b·∫•t k·ª≥ t·∫≠p con n√†o **kh√¥ng thu·ªôc** $L_{k-1}$, lo·∫°i b·ªè $c$.
    *   *V√≠ d·ª•:* X√©t $C_3 = \{A, B, C\}$. C√°c t·∫≠p con l√† $\{A, B\}, \{A, C\}, \{B, C\}$. N·∫øu $\{B, C\}$ kh√¥ng c√≥ trong $L_2$, th√¨ lo·∫°i b·ªè $\{A, B, C\}$.

### 3. Thu·∫≠t To√°n FP-Growth (C·∫•u tr√∫c C√¢y)
*   **Header Table:** B·∫£ng ch·ª©a c√°c item ph·ªï bi·∫øn (ƒë√£ s·∫Øp x·∫øp gi·∫£m d·∫ßn theo support) v√† con tr·ªè ƒë·∫øn node ƒë·∫ßu ti√™n c·ªßa item ƒë√≥ trong c√¢y.
*   **Node-Links:** C√°c li√™n k·∫øt n·ªëi c√°c node c√πng t√™n (c√πng item) tr√™n c√¢y ƒë·ªÉ duy·ªát nhanh khi t√≠nh support.
*   **Conditional Pattern Base:** T·∫≠p h·ª£p c√°c "con ƒë∆∞·ªùng" t·ª´ g·ªëc ƒë·∫øn item ƒëang x√©t (kh√¥ng bao g·ªìm item ƒë√≥).
*   **Conditional FP-Tree:** C√¢y con ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ Conditional Pattern Base. N·∫øu c√¢y n√†y ch·ªâ c√≥ 1 ƒë∆∞·ªùng d·∫´n ƒë∆°n, ta t·ªï h·ª£p c√°c node tr√™n ƒë∆∞·ªùng d·∫´n ƒë·ªÉ ra t·∫≠p ph·ªï bi·∫øn.

---

## üìó Ch∆∞∆°ng 04: Ph√¢n Lo·∫°i (Classification)

### üìñ File tham kh·∫£o:
- [H∆∞·ªõng d·∫´n chi ti·∫øt](chuong-04/huong-dan-chuong-04.md)
- [T·ªïng h·ª£p c√¥ng th·ª©c](chuong-04/tong-hop-cong-thuc.md)

### 1. Naive Bayes & Laplace Smoothing
*   **ƒê·ªãnh l√Ω Bayes:**
    $$P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}$$

*   **Naive Bayes (gi·∫£ ƒë·ªãnh ƒë·ªôc l·∫≠p):**
    $$P(C|X_1, X_2, ..., X_n) \propto P(C) \times \prod_{i=1}^{n} P(X_i|C)$$

*   **V·∫•n ƒë·ªÅ:** N·∫øu m·ªôt gi√° tr·ªã thu·ªôc t√≠nh ch∆∞a t·ª´ng xu·∫•t hi·ªán trong l·ªõp $C$ trong t·∫≠p hu·∫•n luy·ªán, $P(x_i | C) = 0$, d·∫´n ƒë·∫øn x√°c su·∫•t h·∫≠u nghi·ªám b·∫±ng 0.

*   **Kh·∫Øc ph·ª•c (Laplace Correction):**
    $$P(x_i | C) = \frac{Count(x_i, C) + 1}{Count(C) + |V|}$$
    *   $|V|$: S·ªë l∆∞·ª£ng gi√° tr·ªã ph√¢n bi·ªát c·ªßa thu·ªôc t√≠nh $x$ (Vocabulary size).
    *   *V√≠ d·ª•:* Thu·ªôc t√≠nh "M√†u s·∫Øc" c√≥ $\{ƒê·ªè, Xanh, V√†ng\} \rightarrow |V|=3$. N·∫øu l·ªõp "Yes" c√≥ 5 m·∫´u, trong ƒë√≥ kh√¥ng c√≥ m·∫´u n√†o m√†u ƒê·ªè:
        $$P(\text{ƒê·ªè} | \text{Yes}) = \frac{0 + 1}{5 + 3} = \frac{1}{8}$$

### 2. Decision Tree - Entropy & Gain
Cho t·∫≠p $S$ c√≥ $p$ m·∫´u Positive (+) v√† $n$ m·∫´u Negative (-). T·ªïng s·ªë m·∫´u $N = p + n$.
*   **Entropy c·ªßa t·∫≠p $S$:** ƒê·ªô ƒëo s·ª± kh√¥ng ch·∫Øc ch·∫Øn.
    $$Entropy(S) = - \frac{p}{N} \log_2 \left(\frac{p}{N}\right) - \frac{n}{N} \log_2 \left(\frac{n}{N}\right)$$
    *   *L∆∞u √Ω:* N·∫øu $p=0$ ho·∫∑c $n=0$, Entropy = 0 (Ho√†n to√†n thu·∫ßn nh·∫•t). N·∫øu $p=n$, Entropy = 1 (H·ªón lo·∫°n nh·∫•t).
*   **Entropy sau khi chia theo thu·ªôc t√≠nh A:**
    $$Entropy_A(S) = \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \times Entropy(S_v)$$
*   **Information Gain:** M·ª©c ƒë·ªô gi·∫£m Entropy.
    $$Gain(S, A) = Entropy(S) - Entropy_A(S)$$
*   **Gain Ratio (C4.5):**
    $$GainRatio(S, A) = \frac{Gain(S, A)}{SplitInfo(S, A)}$$
*   **Gini Index (CART):**
    $$Gini(S) = 1 - \sum p_i^2$$

### 3. ƒê√°nh Gi√° M√¥ H√¨nh (Confusion Matrix)
Trong b√†i to√°n ph√°t hi·ªán b·ªánh (B·ªánh = Positive, Kh√¥ng b·ªánh = Negative):
*   **Accuracy (ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ):**
    $$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$
*   **Precision (ƒê·ªô ch√≠nh x√°c c·ªßa d·ª± b√°o d∆∞∆°ng):** Trong c√°c ca m√°y ƒëo√°n l√† B·ªánh, bao nhi√™u ca th·ª±c s·ª± B·ªánh?
    $$Precision = \frac{TP}{TP + FP}$$
*   **Recall / Sensitivity (ƒê·ªô nh·∫°y):** Trong c√°c ca th·ª±c s·ª± B·ªánh, m√°y ph√°t hi·ªán ƒë∆∞·ª£c bao nhi√™u?
    $$Recall = \frac{TP}{TP + FN}$$
*   **F1-Score:** C√¢n b·∫±ng gi·ªØa Precision v√† Recall.
    $$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$

---

## üìï Ch∆∞∆°ng 05: Ph√¢n C·ª•m (Clustering)

### üìñ File tham kh·∫£o:
- [H∆∞·ªõng d·∫´n chi ti·∫øt](chuong-05/huong-dan-chuong-05.md)
- [T·ªïng h·ª£p c√¥ng th·ª©c](chuong-05/tong-hop-cong-thuc.md)

### 1. Kho·∫£ng C√°ch (Distance Metrics)
Cho 2 ƒëi·ªÉm $A(x_1, y_1)$ v√† $B(x_2, y_2)$:
*   **Euclidean:** $d = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$
*   **Manhattan:** $d = |x_1-x_2| + |y_1-y_2|$
*   **Minkowski:** $d = (\sum|x_i - y_i|^p)^{1/p}$
*   **Cosine Similarity:** $sim = \frac{A \cdot B}{||A|| \times ||B||}$

### 2. K-Means (C·∫≠p nh·∫≠t tr·ªçng t√¢m)
*   **B∆∞·ªõc Assignment:** G√°n ƒëi·ªÉm $X_i$ v√†o c·ª•m $k$ n·∫øu $d(X_i, C_k)$ l√† nh·ªè nh·∫•t.
*   **B∆∞·ªõc Update Centroid:** T√≠nh l·∫°i t·ªça ƒë·ªô t√¢m c·ª•m $C_k$ m·ªõi b·∫±ng trung b√¨nh c·ªông t·ªça ƒë·ªô c√°c ƒëi·ªÉm thu·ªôc c·ª•m ƒë√≥.
    $$C_k^{new} = \left( \frac{\sum x}{m}, \frac{\sum y}{m} \right)$$
    *   $m$: S·ªë l∆∞·ª£ng ƒëi·ªÉm trong c·ª•m $k$.

### 3. Hierarchical Clustering (C·∫≠p nh·∫≠t ma tr·∫≠n kho·∫£ng c√°ch)
Khi g·ªôp 2 c·ª•m $U$ v√† $V$ th√†nh c·ª•m m·ªõi $(UV)$, kho·∫£ng c√°ch t·ª´ $(UV)$ ƒë·∫øn c·ª•m $W$ b·∫•t k·ª≥ ƒë∆∞·ª£c t√≠nh l·∫°i:
*   **Single Linkage (Min):** $d((UV), W) = \min \{ d(U, W), d(V, W) \}$
*   **Complete Linkage (Max):** $d((UV), W) = \max \{ d(U, W), d(V, W) \}$
*   **Average Linkage:** Trung b√¨nh kho·∫£ng c√°ch c√°c ƒëi·ªÉm.

### 4. DBSCAN (M·∫≠t ƒë·ªô)
*   **Core Point (ƒêi·ªÉm l√µi):** C√≥ √≠t nh·∫•t $MinPts$ ƒëi·ªÉm trong b√°n k√≠nh $\epsilon$.
*   **Border Point (ƒêi·ªÉm bi√™n):** N·∫±m trong b√°n k√≠nh $\epsilon$ c·ªßa Core Point nh∆∞ng kh√¥ng ƒë·ªß $MinPts$.
*   **Noise Point (ƒêi·ªÉm nhi·ªÖu):** Kh√¥ng ph·∫£i Core c≈©ng kh√¥ng ph·∫£i Border.

### 5. ƒê√°nh Gi√° Clustering
*   **Silhouette Score:**
    $$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$
    *   $a(i)$: Kho·∫£ng c√°ch trung b√¨nh trong c√πng cluster
    *   $b(i)$: Kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t ƒë·∫øn cluster kh√°c
*   **WCSS (Within-Cluster Sum of Squares):**
    $$WCSS = \sum_k \sum_{x \in C_k} d(x, centroid_k)^2$$

---

## üìå L∆∞u √ù Quan Tr·ªçng

### Khi L√†m B√†i T·∫≠p:
1. **ƒê·ªçc k·ªπ ƒë·ªÅ b√†i** - X√°c ƒë·ªãnh r√µ y√™u c·∫ßu
2. **X√°c ƒë·ªãnh ph∆∞∆°ng ph√°p** - Ch·ªçn thu·∫≠t to√°n/c√¥ng th·ª©c ph√π h·ª£p
3. **T√≠nh to√°n c·∫©n th·∫≠n** - Ki·ªÉm tra t·ª´ng b∆∞·ªõc
4. **Tr√¨nh b√†y r√µ r√†ng** - Ghi r√µ c√¥ng th·ª©c v√† k·∫øt qu·∫£ trung gian

### C√°c L·ªói Th∆∞·ªùng G·∫∑p:
- **Ch∆∞∆°ng 2:** Qu√™n s·∫Øp x·∫øp d·ªØ li·ªáu tr∆∞·ªõc khi t√≠nh quartiles
- **Ch∆∞∆°ng 3:** Kh√¥ng ki·ªÉm tra ƒë·ªß t·∫•t c·∫£ subsets khi prune
- **Ch∆∞∆°ng 4:** Qu√™n √°p d·ª•ng Laplace smoothing khi c√≥ x√°c su·∫•t = 0
- **Ch∆∞∆°ng 5:** T√≠nh sai kho·∫£ng c√°ch ho·∫∑c kh√¥ng c·∫≠p nh·∫≠t centroid ƒë√∫ng

---

## üéØ Checklist √în T·∫≠p

### Ch∆∞∆°ng 02:
- [ ] T√≠nh ƒë∆∞·ª£c Mean, Median, Mode, Quartiles, Std
- [ ] X√°c ƒë·ªãnh outliers b·∫±ng Boxplot
- [ ] √Åp d·ª•ng 3 ph∆∞∆°ng ph√°p chu·∫©n h√≥a (Min-Max, Z-Score, Decimal Scaling)
- [ ] L√†m m·ªãn d·ªØ li·ªáu b·∫±ng Binning

### Ch∆∞∆°ng 03:
- [ ] T√≠nh Support, Confidence, Lift
- [ ] Ch·∫°y thu·∫≠t to√°n Apriori (Join & Prune)
- [ ] X√¢y d·ª±ng FP-Tree
- [ ] Sinh lu·∫≠t k·∫øt h·ª£p t·ª´ frequent itemsets

### Ch∆∞∆°ng 04:
- [ ] √Åp d·ª•ng Naive Bayes v·ªõi Laplace Smoothing
- [ ] T√≠nh Entropy, Information Gain, Gain Ratio
- [ ] X√¢y d·ª±ng Decision Tree
- [ ] T√≠nh Accuracy, Precision, Recall, F1-Score

### Ch∆∞∆°ng 05:
- [ ] T√≠nh c√°c kho·∫£ng c√°ch (Euclidean, Manhattan, Cosine)
- [ ] Ch·∫°y thu·∫≠t to√°n K-Means
- [ ] X√¢y d·ª±ng Hierarchical Clustering (Dendrogram)
- [ ] √Åp d·ª•ng DBSCAN
- [ ] ƒê√°nh gi√° clustering (Silhouette, WCSS)
