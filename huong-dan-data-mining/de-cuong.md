# ƒê·ªÅ C∆∞∆°ng √în T·∫≠p Data Mining Chi Ti·∫øt

T√†i li·ªáu n√†y t·ªïng h·ª£p l√Ω thuy·∫øt c·ªët l√µi v√† h∆∞·ªõng d·∫´n gi·∫£i chi ti·∫øt c√°c d·∫°ng b√†i t·∫≠p, bao g·ªìm c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát v√† v√≠ d·ª• c·ª• th·ªÉ.

---

## üìò Ch∆∞∆°ng 02: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu (Data Preprocessing)

### 1. Th·ªëng K√™ M√¥ T·∫£ (Descriptive Statistics)
Cho t·∫≠p d·ªØ li·ªáu $X = \{x_1, x_2, ..., x_n\}$ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp tƒÉng d·∫ßn.

*   **Mean (Trung b√¨nh):**
    $$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

*   **Median (Trung v·ªã):** Gi√° tr·ªã n·∫±m gi·ªØa t·∫≠p d·ªØ li·ªáu.
    *   **Tr∆∞·ªùng h·ª£p $n$ l·∫ª:** Median l√† ph·∫ßn t·ª≠ ·ªü v·ªã tr√≠ th·ª© $(n+1)/2$.
        $$Median = x_{(n+1)/2}$$
    *   **Tr∆∞·ªùng h·ª£p $n$ ch·∫µn:** Median l√† trung b√¨nh c·ªông c·ªßa 2 ph·∫ßn t·ª≠ ·ªü gi·ªØa (v·ªã tr√≠ $n/2$ v√† $n/2 + 1$).
        $$Median = \frac{x_{n/2} + x_{n/2 + 1}}{2}$$

*   **Mode (Y·∫øu v·ªã):** Gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu l·∫ßn nh·∫•t trong t·∫≠p d·ªØ li·ªáu.
    *   N·∫øu c√≥ nhi·ªÅu gi√° tr·ªã c√πng t·∫ßn su·∫•t cao nh·∫•t $\rightarrow$ T·∫≠p d·ªØ li·ªáu ƒëa y·∫øu v·ªã (Multimodal).

*   **Quartiles (T·ª© ph√¢n v·ªã):** Chia d·ªØ li·ªáu th√†nh 4 ph·∫ßn b·∫±ng nhau.
    *   **Q2 (Median):** T√≠nh nh∆∞ tr√™n.
    *   **Q1 (T·ª© ph√¢n v·ªã th·ª© nh·∫•t):** Median c·ªßa n·ª≠a ƒë·∫ßu d·ªØ li·ªáu (c√°c gi√° tr·ªã nh·ªè h∆°n Q2).
    *   **Q3 (T·ª© ph√¢n v·ªã th·ª© ba):** Median c·ªßa n·ª≠a sau d·ªØ li·ªáu (c√°c gi√° tr·ªã l·ªõn h∆°n Q2).
    *   *L∆∞u √Ω:* Khi t√≠nh Q1, Q3, n·∫øu $n$ l·∫ª, th∆∞·ªùng kh√¥ng bao g·ªìm Q2 v√†o n·ª≠a ƒë·∫ßu/n·ª≠a sau.

*   **Standard Deviation (ƒê·ªô l·ªách chu·∫©n):**
    $$\sigma = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n}}$$

### 2. Tr·ª±c Quan H√≥a & Outliers (Boxplot)
*   **IQR (Interquartile Range):** Kho·∫£ng tr·∫£i gi·ªØa.
    $$IQR = Q3 - Q1$$
*   **X√°c ƒë·ªãnh Outliers (Ngo·∫°i l·ªá):**
    *   **Bi√™n d∆∞·ªõi (Lower Fence):** $LF = Q1 - 1.5 \times IQR$
    *   **Bi√™n tr√™n (Upper Fence):** $UF = Q3 + 1.5 \times IQR$
    *   B·∫•t k·ª≥ gi√° tr·ªã n√†o $< LF$ ho·∫∑c $> UF$ ƒë·ªÅu l√† Outlier.

### 3. L√†m M·ªãn D·ªØ Li·ªáu (Data Smoothing) - Binning
**Ph∆∞∆°ng ph√°p Bin Means (Trung b√¨nh Bin):**
1.  **S·∫Øp x·∫øp** d·ªØ li·ªáu tƒÉng d·∫ßn.
2.  **Ph√¢n chia (Partition):** Chia d·ªØ li·ªáu v√†o c√°c bin (th√πng).
    *   *Equal-depth (frequency):* M·ªói bin c√≥ s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ b·∫±ng nhau. (Th∆∞·ªùng d√πng).
    *   *Equal-width:* Kho·∫£ng gi√° tr·ªã m·ªói bin b·∫±ng nhau.
3.  **L√†m m·ªãn:** Thay th·∫ø T·∫§T C·∫¢ gi√° tr·ªã trong m·ªói bin b·∫±ng gi√° tr·ªã TRUNG B√åNH c·ªßa bin ƒë√≥.

### 4. Chu·∫©n H√≥a D·ªØ Li·ªáu (Normalization)

*   **Min-Max Normalization:** ƒê∆∞a gi√° tr·ªã $v$ v·ªÅ kho·∫£ng $[new\_min, new\_max]$ (th∆∞·ªùng l√† $[0, 1]$).
    $$v' = \frac{v - min_{old}}{max_{old} - min_{old}} \times (new\_max - new\_min) + new\_min$$
    *   *V√≠ d·ª•:* $v=35, min=13, max=70$, ƒë∆∞a v·ªÅ $[0, 1]$.
        $$v' = \frac{35 - 13}{70 - 13} \times (1 - 0) + 0 = \frac{22}{57} \approx 0.386$$

*   **Z-Score Normalization:** D√πng khi kh√¥ng bi·∫øt r√µ min/max ho·∫∑c c√≥ outliers.
    $$v' = \frac{v - \mu}{\sigma}$$
    *   $\\mu$: Trung b√¨nh.
    *   $\\sigma$: ƒê·ªô l·ªách chu·∫©n.
    *   *Bi·∫øn th·ªÉ (D√πng Mean Absolute Deviation - MAD):* $v' = \frac{v - \mu}{MAD}$ v·ªõi $MAD = \frac{1}{n}\\sum |x_i - \bar{x}|$.

*   **Decimal Scaling:** Di chuy·ªÉn d·∫•u ph·∫©y ƒë·ªông.
    $$v' = \frac{v}{10^j}$$
    *   $j$ l√† s·ªë nguy√™n nh·ªè nh·∫•t sao cho gi√° tr·ªã tuy·ªát ƒë·ªëi l·ªõn nh·∫•t c·ªßa t·∫≠p d·ªØ li·ªáu sau khi chia nh·ªè h∆°n 1 ($|v'| < 1$).
    *   *V√≠ d·ª•:* T·∫≠p d·ªØ li·ªáu c√≥ gi√° tr·ªã l·ªõn nh·∫•t l√† 980 $\\rightarrow$ Chia cho 1000 ($j=3$) $\\rightarrow 0.98$.

---

## üìô Ch∆∞∆°ng 03: Lu·∫≠t K·∫øt H·ª£p (Association Rules)

### 1. Thu·∫≠t To√°n Apriori (Chi ti·∫øt Join & Prune)
T√¨m t·∫≠p ph·ªï bi·∫øn v·ªõi $min\_sup$.
*   **B∆∞·ªõc Join ($L_{k-1} \bowtie L_{k-1}$):**
    *   K·∫øt h·ª£p hai t·∫≠p m·ª•c trong $L_{k-1}$ n·∫øu ch√∫ng gi·ªëng nhau ·ªü $k-2$ ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.
    *   *V√≠ d·ª•:* $L_2 = \{ \{A, B\}, \{A, C\}, \{A, E\}, \{B, C\} \}$
        *   Join $\{A, B\}$ v√† $\{A, C\} \rightarrow \{A, B, C\}$ (Candidate $C_3$).
        *   Join $\{A, B\}$ v√† $\{A, E\} \rightarrow \{A, B, E\}$.
*   **B∆∞·ªõc Prune (C·∫Øt t·ªâa):**
    *   V·ªõi m·ªói candidate $c \in C_k$, ki·ªÉm tra T·∫§T C·∫¢ t·∫≠p con k√≠ch th∆∞·ªõc $k-1$ c·ªßa n√≥.
    *   N·∫øu c√≥ b·∫•t k·ª≥ t·∫≠p con n√†o **kh√¥ng thu·ªôc** $L_{k-1}$, lo·∫°i b·ªè $c$.
    *   *V√≠ d·ª•:* X√©t $C_3 = \{A, B, C\}$. C√°c t·∫≠p con l√† $\{A, B\}, \{A, C\}, \{B, C\}$. N·∫øu $\{B, C\}$ kh√¥ng c√≥ trong $L_2$, th√¨ lo·∫°i b·ªè $\{A, B, C\}$.

### 2. Thu·∫≠t To√°n FP-Growth (C·∫•u tr√∫c C√¢y)
*   **Header Table:** B·∫£ng ch·ª©a c√°c item ph·ªï bi·∫øn (ƒë√£ s·∫Øp x·∫øp gi·∫£m d·∫ßn theo support) v√† con tr·ªè ƒë·∫øn node ƒë·∫ßu ti√™n c·ªßa item ƒë√≥ trong c√¢y.
*   **Node-Links:** C√°c li√™n k·∫øt n·ªëi c√°c node c√πng t√™n (c√πng item) tr√™n c√¢y ƒë·ªÉ duy·ªát nhanh khi t√≠nh support.
*   **Conditional Pattern Base:** T·∫≠p h·ª£p c√°c "con ƒë∆∞·ªùng" t·ª´ g·ªëc ƒë·∫øn item ƒëang x√©t (kh√¥ng bao g·ªìm item ƒë√≥).
*   **Conditional FP-Tree:** C√¢y con ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ Conditional Pattern Base. N·∫øu c√¢y n√†y ch·ªâ c√≥ 1 ƒë∆∞·ªùng d·∫´n ƒë∆°n, ta t·ªï h·ª£p c√°c node tr√™n ƒë∆∞·ªùng d·∫´n ƒë·ªÉ ra t·∫≠p ph·ªï bi·∫øn.

---

## üìó Ch∆∞∆°ng 04: Ph√¢n Lo·∫°i (Classification)

### 1. Naive Bayes & Laplace Smoothing
*   **V·∫•n ƒë·ªÅ:** N·∫øu m·ªôt gi√° tr·ªã thu·ªôc t√≠nh ch∆∞a t·ª´ng xu·∫•t hi·ªán trong l·ªõp $C$ trong t·∫≠p hu·∫•n luy·ªán, $P(x_i | C) = 0$, d·∫´n ƒë·∫øn x√°c su·∫•t h·∫≠u nghi·ªám b·∫±ng 0.
*   **Kh·∫Øc ph·ª•c (Laplace Correction):**
    $$P(x_i | C) = \frac{Count(x_i, C) + 1}{Count(C) + |V|}$$
    *   $|V|$: S·ªë l∆∞·ª£ng gi√° tr·ªã ph√¢n bi·ªát c·ªßa thu·ªôc t√≠nh $x$ (Vocabulary size).
    *   *V√≠ d·ª•:* Thu·ªôc t√≠nh "M√†u s·∫Øc" c√≥ $\{ƒê·ªè, Xanh, V√†ng\} \rightarrow |V|=3$. N·∫øu l·ªõp "Yes" c√≥ 5 m·∫´u, trong ƒë√≥ kh√¥ng c√≥ m·∫´u n√†o m√†u ƒê·ªè:
        $$P(\text{ƒê·ªè} | \text{Yes}) = \frac{0 + 1}{5 + 3} = \frac{1}{8}$$

### 2. Decision Tree - Entropy & Gain
Cho t·∫≠p $S$ c√≥ $p$ m·∫´u Positive (+) v√† $n$ m·∫´u Negative (-). T·ªïng s·ªë m·∫´u $N = p + n$.
*   **Entropy c·ªßa t·∫≠p $S$:** ƒê·ªô ƒëo s·ª± kh√¥ng ch·∫Øc ch·∫Øn.
    $$Entropy(S) = - \frac{p}{N} \log_2 \left(\frac{p}{N}\right) - \frac{n}{N} \log_2 \left(\frac{n}{N}\right)$$
    *   *L∆∞u √Ω:* N·∫øu $p=0$ ho·∫∑c $n=0$, Entropy = 0 (Ho√†n to√†n thu·∫ßn nh·∫•t). N·∫øu $p=n$, Entropy = 1 (H·ªón lo·∫°n nh·∫•t).
*   **Entropy sau khi chia theo thu·ªôc t√≠nh A:**
    $$Entropy_A(S) = \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \times Entropy(S_v)$$
*   **Information Gain:** M·ª©c ƒë·ªô gi·∫£m Entropy.
    $$Gain(S, A) = Entropy(S) - Entropy_A(S)$$

### 3. ƒê√°nh Gi√° M√¥ H√¨nh (Confusion Matrix)
Trong b√†i to√°n ph√°t hi·ªán b·ªánh (B·ªánh = Positive, Kh√¥ng b·ªánh = Negative):
*   **Accuracy (ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ):** T·ª∑ l·ªá ƒëo√°n ƒë√∫ng tr√™n t·ªïng s·ªë.
*   **Precision (ƒê·ªô ch√≠nh x√°c c·ªßa d·ª± b√°o d∆∞∆°ng):** Trong c√°c ca m√°y ƒëo√°n l√† B·ªánh, bao nhi√™u ca th·ª±c s·ª± B·ªánh? (Quan tr·ªçng khi chi ph√≠ b√°o ƒë·ªông gi·∫£ cao).
    $$Precision = \frac{TP}{TP + FP}$$
*   **Recall / Sensitivity (ƒê·ªô nh·∫°y):** Trong c√°c ca th·ª±c s·ª± B·ªánh, m√°y ph√°t hi·ªán ƒë∆∞·ª£c bao nhi√™u? (Quan tr·ªçng trong y t·∫ø, kh√¥ng mu·ªën b·ªè s√≥t b·ªánh).
    $$Recall = \frac{TP}{TP + FN}$$
*   **F1-Score:** C√¢n b·∫±ng gi·ªØa Precision v√† Recall.

---

## üìï Ch∆∞∆°ng 05: Ph√¢n C·ª•m (Clustering)

### 1. Kho·∫£ng C√°ch (Distance Metrics)
Cho 2 ƒëi·ªÉm $A(x_1, y_1)$ v√† $B(x_2, y_2)$:
*   **Euclidean:** $d = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$
*   **Manhattan:** $d = |x_1-x_2| + |y_1-y_2|$

### 2. K-Means (C·∫≠p nh·∫≠t tr·ªçng t√¢m)
*   **B∆∞·ªõc Assignment:** G√°n ƒëi·ªÉm $X_i$ v√†o c·ª•m $k$ n·∫øu $d(X_i, C_k)$ l√† nh·ªè nh·∫•t.
*   **B∆∞·ªõc Update Centroid:** T√≠nh l·∫°i t·ªça ƒë·ªô t√¢m c·ª•m $C_k$ m·ªõi b·∫±ng trung b√¨nh c·ªông t·ªça ƒë·ªô c√°c ƒëi·ªÉm thu·ªôc c·ª•m ƒë√≥.
    $$C_k^{new} = \left( \frac{\sum x}{m}, \frac{\sum y}{m} \right)$$
    *   $m$: S·ªë l∆∞·ª£ng ƒëi·ªÉm trong c·ª•m $k$.

### 3. Hierarchical Clustering (C·∫≠p nh·∫≠t ma tr·∫≠n kho·∫£ng c√°ch)
Khi g·ªôp 2 c·ª•m $U$ v√† $V$ th√†nh c·ª•m m·ªõi $(UV)$, kho·∫£ng c√°ch t·ª´ $(UV)$ ƒë·∫øn c·ª•m $W$ b·∫•t k·ª≥ ƒë∆∞·ª£c t√≠nh l·∫°i:
*   **Single Linkage (Min):** $d((UV), W) = \min \{ d(U, W), d(V, W) \}$
*   **Complete Linkage (Max):** $d((UV), W) = \max \{ d(U, W), d(V, W) \}$
*   **Average Linkage:** Trung b√¨nh kho·∫£ng c√°ch c√°c ƒëi·ªÉm.

### 4. DBSCAN (M·∫≠t ƒë·ªô)
*   **Core Point (ƒêi·ªÉm l√µi):** C√≥ √≠t nh·∫•t $MinPts$ ƒëi·ªÉm trong b√°n k√≠nh $\\epsilon$.
*   **Border Point (ƒêi·ªÉm bi√™n):** N·∫±m trong b√°n k√≠nh $\\epsilon$ c·ªßa Core Point nh∆∞ng kh√¥ng ƒë·ªß $MinPts$.
*   **Noise Point (ƒêi·ªÉm nhi·ªÖu):** Kh√¥ng ph·∫£i Core c≈©ng kh√¥ng ph·∫£i Border.
